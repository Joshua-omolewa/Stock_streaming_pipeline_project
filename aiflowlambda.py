#created by joshua Omolewa

#importing important libraries
import airflow
from airflow import DAG
from airflow.operators.python_operator import PythonOperator #importing python operator to use python fucntions
from airflow.exceptions import AirflowException #importing aiflowexception to cause the task to move immediately to failure state.
from datetime import datetime, timedelta
import boto3 #library used to communicate with aws services
import requests
import json
import time

default_args = {
    'owner': 'joshua_data_engineer',
    'depends_on_past': False,
    'start_date': airflow.utils.dates.days_ago(0),   #defining when airflow starts executing the task 
    'email': ['airflow_info@gmail.com'],  #email nottifcation settings
    'email_on_failure': False,
    'email_on_retry': False
}

dag = DAG(
    'invoke_lambda_and_check_batch_status', #name of dag/dag id
    default_args=default_args,
    description='Invoke AWS Lambda function and check Livy batch status',
    schedule_interval=None #defining shedule interval for task, None is situable for manually triggered job via Airflow REST API
)

def invoke_lambda_function(**context):
    """
    This function uses Boto3 library to invoke an AWS Lambda function named streamjob-kafka with an empty payload. 
    It then extracts the batch ID from the Lambda function's response payload 
    and stores it as an XCom variable using the context['task_instance'].xcom_push() method.

    **context is a special keyword argument (dictionary) that provides access to various Airflow-related objects and information during task execution. 
    It is a dictionary that includes information such as the TaskInstance object, the DAG object, and execution dates and timestamps.
    """

    client = boto3.client('lambda', region_name='us-east-1')
    response = client.invoke(
        FunctionName='streamjob-kafka',
        InvocationType='RequestResponse',
        Payload='{}'
    )
    payload = response['Payload'].read().decode('utf-8')
    batch_id = extract_batch_id(payload)
    context['task_instance'].xcom_push(key='batch_id', value=batch_id)

def extract_batch_id(payload):
    """ 
    This function extract the batch id from the Lambda function payload generated by runing the livy command
    and return the batch id
    """
    payload_data = json.loads(payload)
    body_data = json.loads(payload_data['body'])
    id_value = body_data['id']
    return id_value


def check_batch_status(**context):
    """
    This function checks the status of the Livy batch using the batch ID stored in the XCom variable. 
    It sends a GET request to a Livy endpoint to retrieve the batch status and waits for the session state 
    to change to either 'shutting_down', 'error', 'dead', 'killed', 'success', 'cancelling', or 'cancelled'. 
    If the session state is anything other than one of these states, it sleeps for 5 seconds and retries.
    If the session state is 'shutting_down', 'error', 'dead', 'killed', 'cancelling', or 'cancelled', it raises an AirflowException with an appropriate error message. 
    Otherwise, it prints a message indicating that the Livy batch session state is successful.
    """
    livy_url = 'http://ec2-34-207-210-96.compute-1.amazonaws.com:8998/batches/{0}'.format(context['task_instance'].xcom_pull(task_ids='invoke_lambda', key='batch_id'))
    
    session_state = ''
    while session_state not in ['shutting_down', 'error', 'dead', 'killed', 'success','cancelling','cancelled']:
        response = requests.get(livy_url)
        session_state = response.json()['state']
        print("Session state: ", session_state)
        if session_state not in ['shutting_down', 'error', 'dead', 'killed', 'success', 'cancelling','cancelled']:
            time.sleep(5) #to make the function wait before going back to the while loop
    
    if session_state in ['shutting_down', 'error', 'dead', 'killed', 'cancelling','cancelled']:
        raise AirflowException("Livy batch session state is {0}".format(session_state)) #aiflowexception to cause the task to move immediately to failure state.
    else:
        print("Livy batch session state is {0}".format(session_state))


#TASKS TO BE EXECUTED BY AIRFLOW

#TASK 1 : invoke lambda function in AWS

invoke_lambda = PythonOperator(
    task_id='invoke_lambda',
    provide_context=True,
    python_callable=invoke_lambda_function,
    dag=dag,
)

#TASK 2 : check batch status of job runing in livy

check_batch = PythonOperator(
    task_id='check_batch_status',
    provide_context=True,
    python_callable=check_batch_status,
    dag=dag,
)

#SETTING THE DEPENDECIES ( This instruct aiflow  on the task dependencies)
invoke_lambda >> check_batch
